{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AdamW\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sentencepiece\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(int(1e10))\n",
    "\n",
    "column_names = ['Article_Text', 'Links']\n",
    "df = pd.read_csv('data.csv', delimiter='\\u2063', names=column_names, engine='python')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Size:\", df.shape[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_truncate(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    truncated_tokens = tokens[:max_length]\n",
    "    return truncated_tokens\n",
    "\n",
    "def truncate_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    truncated_tokens = tokens[:max_length]\n",
    "    return tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "\n",
    "def check_words(row):\n",
    "    text = row['Article_Text']\n",
    "    links_list = row['LinksList']\n",
    "    filtered_links = [word for word in links_list if word in text]\n",
    "    return filtered_links\n",
    "\n",
    "df['Article_Text'] = df['Article_Text'].apply(truncate_text)\n",
    "df['Tokenized_Text'] = df['Article_Text'].apply(tokenize_and_truncate)\n",
    "\n",
    "df = df[df['Links'].apply(lambda x: isinstance(x, str))]\n",
    "df.head()\n",
    "df['LinksList'] = df['Links'].apply(lambda x: list(set(x.split(', '))))\n",
    "df['FilteredLinksList'] = df.apply(check_words, axis=1)\n",
    "df['Links'] = df['FilteredLinksList'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, source_tokens, target_text, max_length=max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_tokens = source_tokens\n",
    "        self.target_text = target_text\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_tokens = self.data.loc[index, self.source_tokens]\n",
    "        target_text = str(self.data.loc[index, self.target_text])\n",
    "\n",
    "        # Encoding the source and target text\n",
    "        inputs = self.tokenizer(\n",
    "            text=source_tokens,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            text=target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Formatting the inputs and targets\n",
    "        item = {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze(),\n",
    "            \"decoder_attention_mask\": targets[\"attention_mask\"].squeeze()\n",
    "        }\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer, 'Article_Text', 'Links', max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"results/t5\"\n",
    "# Check whether the specified path exists or not\n",
    "exists = os.path.exists(path)\n",
    "if not exists:\n",
    "   os.makedirs(path)\n",
    "   print(\"The new directory is created!\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{10}\", leave=True):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model.save_pretrained(path + f'/epoch_{epoch+1}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
